# From Chapter 3 Exercise 2 in Real Econometrics by Michael Bailery, first edition
# recreation of regression simulation
# of salary regressed on education level in years

# additional info on set-up:
# Setting Simulation and model parameters
# The true model will be:
# Salary = 12000 + 1000x + err
# with years of education = 0, expect salary of 12k
# each additional year of education increases salary by 1k
# so a undergraduate degree should give you a an
# additional 16k in salary
# remember, the error terms captures all of factors
# not included in the model that affect Salary
library(tidyverse)
library(ggthemes)

run_simulation <- function(obs_num, sim_num, std_err){
  # Set up model and simulation parameters:
  observations <- obs_num # observations in dataframe
  simulation_reps <- sim_num # number of times we run simulations
  true_beta_0 <- 12000
  true_beta_1 <- 1000
  err_std_deviation <- std_err
  # generating x values for the model
  education <- 1:16 %>% 
    sample(size=observations, replace = TRUE) # My preferred way of setting Education
  
  # this matrix will store the estimated coefficients
  # generated by each simulation
  coef_matrix <- matrix(data = NA,nrow = simulation_reps,ncol = 2)
  # begin simulation
  for (i in 1:simulation_reps){
    # generating y values with our model
    # our model has the true value, but
    # there is still inherent randomness to data generation aka 'noise'
    # this is captured in the error term
    # we'll mode this by setting the error to be the standard deviation of the error * rnorm(n=observations)
    # creating our vector our salary variable(denoted as y)
    # our eplison vector composed of the passed in value times a value from the standard normal distribution
    y <- true_beta_0 + true_beta_1*education + (err_std_deviation*rnorm(n = observations))
    
    # creating an ols model from our x and y vector:
    ols_results <- lm(y ~ education)
    # put coefficients, b_0 and b_1 in ith row of matrix
    coef_matrix[i,] <- coefficients(ols_results)
    
    # scatterplot of x and y with line of best fit from ols results
    plot(jitter(x = education,1), y = y,pch = 19, col= 'steelblue')
    # plot(x = education, y = y,pch = 19, col= 'steelblue')
    abline(ols_results,lwd = 3, col= "darkgreen")
    Sys.sleep(0.075)
  }
  # running summary statistics on estimated beta in coef_matrix
  beta_0_stats <- c(mean(coef_matrix[,1]), min(coef_matrix[,1]), max(coef_matrix[,1]))  
  # Average, min and max of beta_0 estimates
  print(paste0("basic stats on beta_0_hat: mean, min, max ", beta_0_stats))
  
  beta_1_stats <- c(mean(coef_matrix[,2]), min(coef_matrix[,2]), max(coef_matrix[,2]))  
  # Average, min and max of beta_1 estimates
  print(paste0("basic stats on beta_1_hat: mean, min, max ", beta_1_stats))
  
  # consider a println after colon :
  beta_hat_tibble <- tibble(beta_hat_0 = beta_0_stats, beta_hat_1 = beta_1_stats)
  
  beta_hat_tibble
  
  # Kernel Density Plot
  plot(density(coef_matrix[,2]), main = 'Kernel Density Estimate')
}

run_simulation(obs_num = 100, sim_num = 50, std_err = 10000)



# a) Explain why the means of the estimated coeff across
# the multiple simulations are what they are.
# *best to answer this after running
# b-g

# basically, the difference in means of the beta coefficients
# across the different simulations is dependent on
# 1) the sample size of the dataset(larger set = more accurate results)
# a lot to do with the central limit theorem and its relation to the
# distribution of estimated coefficients
# 2) the value of the standard deviation
# a smaller std, and we have less 'noise' in the simulation,
# also aiding in more accurate results
# 3) (maybe???) the ratio between number of observations and number of repettions
# average values seem less accurate when the number repettions in the simulation
# is far greater than the size of the dataset



# b) What are the min and max values of the estimated coefficients 
# on education?
# Are these values inconsistent with our statement that
# OLS estimates are unbiased
# note for an unbiased estimated Ex(est_beta_i) = beta_i

# answer: min coeffcient on beta_hat_1: 650.2779
# max coeffcient on beta_hat_1: 1507.676
# from our simmulation resutls, we can conclude that
# at a minimum, our regression will estimate that
# a 1 yr increase in education is correlated with an
# additional 650.3 dollar in yearly salary,
# our max is that a 1 yr increase gets a 1507.7 dollars increase in yearly salary

# we know that our beta_hat_1 is unbiased if the 
# average beta_hat_1 of the distribution is close
# to the true value,
# in the distribution, it is still possible to get
# a value that is magnitudes off from the true value(extreme max or min),
# so seeing these values doesn't necessarily tell us
# our beta_1 estimate is biased.
# looking at the mean beta_hat_1 of 998.1 dollars,
# this is really close to the true value of 1000,
# so our beta_hat_1 is an unbiased estimator

# c) Rerun sim with a large sample size,
# specifically, set the sample size to 1000
# compare mean, max, min of estimated coefficients on education
# to the above
run_simulation(obs_num = 1000, sim_num = 50, std_err = 10000)

# answer: 
# mean beta_hat_1: 997.67
# max beta_hat_1: 824.65
# min beta_hat_1: 1149.10

# Notice that with a large sample size(data set contains 1000 observations compared to 100),
# the distribution of the beta_hat_1 becomes more narrow,
# the min and max values are closer to the mean
# and as this mean beta_hat_1 is still very close to
# the true value of the coefficient on education,
# consequently, the min and max values are closer to the true value

# d) Rerun sim with a smaller sample size,
# specifically, set sample size to 20.
# compare mean, min , and max to original results from b)
run_simulation(obs_num = 20,sim_num = 50,std_err = 10000)

# answer: now we can see the limitations of a small sample size,
# the distribution of the estimate beta_1 is much wider now,
# the min and max values are nowhere near the true value of the coefficient on education
# on the min end, we are evening seeing a negative correlation
# between education years and salary
# now our beta_1_hat mean is greater than the true value
# ,about 26 dollars larger, while not extremely close
# like in the above results, it's still not a terrible estimate
# so I think we can conclude that our estimated beta_1 is still
# not a biased estimator, since the avg value is pretty close
# even though the extreme values are now nowhere near close

# e) Reset sample size to 100, but decrease value on std. deviation
# specifically, set the stddev to be 500
# compare again, to original results from b)

run_simulation(obs_num = 100, sim_num = 50,std_err = 500)

# answer: two things are immediately noticeable,
# with a small std_err, there is less noise, so the
# scatterplot depiction is almost a perfect linear relationship
# the beta_hat_1 distribution is the most narrow,
# perhaps we can conclude that a smaller std_err has
# a larger impact on the distribution of beta_hat_1
# over the obs_num size

# here, the min and max are much closer to the true value
# compared to when standard deviation was 10,000
# noticeably, the mean beta_hat_1 is still very close
# to the one where std. was 10,000

#f) keep sample size at 100, but rerun with a large std. deviation,
# specifically, set std. dev to 50,000
# compare results to original from b)

run_simulation(obs_num = 100,sim_num = 50,std_err = 50000)

# answer: this gave us the widest distribution for 
# our beta_1 estimate, the max and min are now extreme values
# not close to the true value.

# similar to how a small sample size over estimated
# the value of the coefficient on education, so too did
# this model where the std deviation is very large
# to me, this may show us that a smaller sample size is 
# going to start producing estimates further away from the
# true value "quicker" than an increase in standard error
# we (probably) wouldn't expect to see such a massive standard error,
# it's a value close to most median incomes in the U.S.

# maybe this is important for research,
# focus more on collecting observations first than minimizing the
# effect of the standard deviation of the error term,
#  it takes a much larger value for the former(standard error) to start
# messing with the consistency of the estimates.

# g) Revert to original mode, but change reps to 500.
# Summarize results on education coefficient and compare to b)
# now also plot the distribution of the coefficients 
# using the kde plot

run_simulation(obs_num = 100, sim_num = 500, std_err = 10000)

# increasing the number of simulations actually 
# decreased the accuracy across all measurements: mean, min and max
# to be consistent estimators to the true values on the coefficients

# I think this has something to do with 
# the relationship between the sample size of the data
# and the simulation reps. 

# We have 5x as many reps compared to observations in the dataset.
#previously, for the original problem b),we had double the observations(100) of reps(50)

# so something about a wide magnitude between
# the two values creates some kind of 'skewedness' to the
# estimations of the coefficients?

# I might be reaching here.
